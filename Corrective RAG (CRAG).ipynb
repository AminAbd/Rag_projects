{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Project Notebook\n",
        "\n",
        "This notebook contains code for the RAG (Retrieval-Augmented Generation) project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Corrective RAG (CRAG)**\n",
        "\n",
        "Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents.\n",
        "\n",
        "In the paper here, a few steps are taken:\n",
        "\n",
        "If at least one document exceeds the threshold for relevance, then it proceeds to generation\n",
        "Before generation, it performs knowledge refinement\n",
        "This partitions the document into \"knowledge strips\"\n",
        "It grades each strip, and filters our irrelevant ones\n",
        "If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource\n",
        "It will use web search to supplement retrieval\n",
        "We will implement some of these ideas from scratch using LangGraph:\n",
        "\n",
        "Let's skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired.\n",
        "If any documents are irrelevant, let's opt to supplement retrieval with web search.\n",
        "We'll use Tavily Search for web search.\n",
        "Let's use query re-writing to optimize the query for web search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Setup**\n",
        "\n",
        "First, let's download our required packages and set our API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file with UTF-8 encoding\n",
        "load_dotenv(encoding='utf-8')\n",
        "\n",
        "# Get the API key from environment\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Create Index**\n",
        "Let's index 3 blog posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "1# Install langchainhub if not already installed\n",
        "%pip install langchainhub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "### Retrieval Grader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "#from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "#docs = retriever.get_relevant_documents(question)\n",
        "docs = retriever.invoke(question)\n",
        "\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The agent system overview includes a memory component that involves long-term memory management and recording experiences in natural language. The memory module serves as an external database for the agents to access past experiences and interactions. The agents can use memory for reflection, planning, and learning from past actions to improve future performance.\n"
          ]
        }
      ],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt - RAG prompt template (equivalent to rlm/rag-prompt from hub)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"),\n",
        "    (\"human\", \"Question: {question}\\n\\nContext: {context}\")\n",
        "])\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain - includes retrieval, formatting, prompt, LLM, and output parsing\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": RunnableLambda(lambda x: x[\"question\"]) | retriever | format_docs,\n",
        "        \"question\": RunnableLambda(lambda x: x[\"question\"])\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What is the role of memory in artificial intelligence agents?'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Web Search Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ameen\\AppData\\Local\\Temp\\ipykernel_25140\\730760015.py:5: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
            "  web_search_tool = TavilySearchResults(k=3)\n"
          ]
        }
      ],
      "source": [
        "### Search\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Create Graph**\n",
        "\n",
        "Now let's create our graph that will use CRAG\n",
        "\n",
        "Define Graph State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search: str\n",
        "    documents: List[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            web_search = \"Yes\"\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results)\n",
        "\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    web_search = state[\"web_search\"]\n",
        "    state[\"documents\"]\n",
        "\n",
        "    if web_search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Compile Graph**\n",
        "The just follows the flow we outlined in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
        "workflow.add_edge(\"web_search_node\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---TRANSFORM QUERY---\n",
            "\"Node 'transform_query':\"\n",
            "'\\n---\\n'\n",
            "---WEB SEARCH---\n",
            "\"Node 'web_search_node':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('Agents possess different types of memory, including sensory memory for '\n",
            " 'retaining sensory information briefly, short-term memory for in-context '\n",
            " 'learning and complex cognitive tasks, and long-term memory for storing '\n",
            " 'information over extended periods. Long-term memory can further be '\n",
            " 'categorized into explicit (declarative) memory for facts and events, and '\n",
            " 'implicit (procedural) memory for skills and routines. Agents also utilize '\n",
            " 'memory streams as a long-term memory module to record experiences in natural '\n",
            " 'language.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---TRANSFORM QUERY---\n",
            "\"Node 'transform_query':\"\n",
            "'\\n---\\n'\n",
            "---WEB SEARCH---\n",
            "\"Node 'web_search_node':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "(\"I don't have information on the specific mechanisms of the AlphaCodium paper \"\n",
            " 'or how it functions.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"How does the AlphaCodium paper work?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
