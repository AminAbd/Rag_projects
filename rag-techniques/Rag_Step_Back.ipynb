{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269d9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # reads .env and injects into environment\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "# retrieve from environment\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6871cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c031111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5a1f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the process of breaking down tasks for LLM agents?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8375385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals. This allows the agent to efficiently handle intricate tasks by dividing them into smaller steps that are easier to tackle. Task decomposition is a crucial component of planning for LLM-powered autonomous agents, as it enables them to navigate through complicated tasks effectively.\\n\\nThere are various techniques and approaches to task decomposition for LLM agents. One common method is the use of prompting techniques such as Chain of Thought (CoT) and Tree of Thoughts. CoT instructs the model to \"think step by step\" and decompose hard tasks into smaller and simpler steps, transforming big tasks into multiple manageable tasks. Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought steps and generating multiple thoughts per step. The search process can be conducted using BFS or DFS, with each state evaluated by a classifier or majority vote.\\n\\nTask decomposition can also be achieved through different means, such as simple prompting by the LLM, task-specific instructions, or human inputs. For example, LLM agents can be prompted with instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ\" to guide them through the task decomposition process. Additionally, task-specific instructions can be provided, such as \"Write a story outline\" for writing a novel, to help the agent break down the task into manageable subgoals.\\n\\nOverall, task decomposition is a fundamental aspect of planning for LLM agents, allowing them to effectively navigate complex tasks by breaking them down into smaller, more manageable steps. By utilizing techniques like CoT and Tree of Thoughts, along with various prompting methods, LLM agents can enhance their performance and problem-solving capabilities in a wide range of domains.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
